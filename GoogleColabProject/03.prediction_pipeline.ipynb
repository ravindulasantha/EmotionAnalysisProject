{"cells":[{"cell_type":"code","execution_count":3,"id":"6f496715-5093-42c3-9481-d236a6ce1d8b","metadata":{"id":"6f496715-5093-42c3-9481-d236a6ce1d8b","executionInfo":{"status":"ok","timestamp":1692419810681,"user_tz":-330,"elapsed":3,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import string\n","import pickle"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCiqVEOoFI0o","executionInfo":{"status":"ok","timestamp":1692419810681,"user_tz":-330,"elapsed":19223,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}},"outputId":"ce6ff8a4-5c0f-4882-9dfe-2cefd4235888"},"id":"VCiqVEOoFI0o","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":1,"id":"31185b08-a39e-4e1a-8be2-8cdbfe6722bb","metadata":{"id":"31185b08-a39e-4e1a-8be2-8cdbfe6722bb","executionInfo":{"status":"ok","timestamp":1692419773699,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}}},"outputs":[],"source":["def remove_punctuations(text):\n","    for punctuation in string.punctuation:\n","        text = text.replace(punctuation, '')\n","    return text"]},{"cell_type":"code","execution_count":5,"id":"35f94b9c-d620-4ee1-a8ab-5942eebd9846","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35f94b9c-d620-4ee1-a8ab-5942eebd9846","executionInfo":{"status":"ok","timestamp":1692419891681,"user_tz":-330,"elapsed":1900,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}},"outputId":"b05a135b-4552-428c-9c3d-9c31654098e1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]}],"source":["import pickle\n","\n","# Update the file path to the correct one\n","file_path = '/content/drive/MyDrive/EmotionAnalysisProject-main/static/model/model.pickle'\n","\n","# Load the pickled model\n","with open(file_path, 'rb') as f:\n","    model = pickle.load(f)\n"]},{"cell_type":"code","execution_count":9,"id":"4d49a458-30ad-4ea1-9147-f3a1d059971a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4d49a458-30ad-4ea1-9147-f3a1d059971a","executionInfo":{"status":"ok","timestamp":1692420010123,"user_tz":-330,"elapsed":546,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}},"outputId":"3e3d4f50-c847-491a-99ab-ce107aa68a0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Error: The specified file does not exist.\n"]}],"source":["# Replace this with the actual absolute path to your file\n","file_path = '/content/drive/MyDrive/EmotionAnalysisProject-main/static/model/corpora/stopwords/english.txt'\n","\n","try:\n","    with open(file_path, 'r') as file:\n","        sw = file.read().splitlines()\n","    print(\"Stopwords loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"Error: The specified file does not exist.\")\n"]},{"cell_type":"code","execution_count":11,"id":"7e29f577-7cad-4503-ba28-3201b43d9ed3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7e29f577-7cad-4503-ba28-3201b43d9ed3","executionInfo":{"status":"ok","timestamp":1692420055953,"user_tz":-330,"elapsed":907,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}},"outputId":"444fe72a-df1f-4f13-9c8a-c1b646f39230"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary loaded successfully.\n"]}],"source":["file_path = '/content/drive/MyDrive/EmotionAnalysisProject-main/static/model/vocabulary.txt'\n","\n","try:\n","    vocab = pd.read_csv(file_path, header=None)\n","    tokens = vocab[0].tolist()\n","    print(\"Vocabulary loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"Error: The specified file does not exist.\")\n"]},{"cell_type":"code","execution_count":12,"id":"d42b4fcf-4c42-4db4-8a28-06aafce43312","metadata":{"id":"d42b4fcf-4c42-4db4-8a28-06aafce43312","executionInfo":{"status":"ok","timestamp":1692420061518,"user_tz":-330,"elapsed":569,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}}},"outputs":[],"source":["from nltk.stem import PorterStemmer\n","ps = PorterStemmer()"]},{"cell_type":"code","execution_count":34,"id":"36f13705-f541-4fdf-bb2d-c8a350fd4dd5","metadata":{"id":"36f13705-f541-4fdf-bb2d-c8a350fd4dd5","executionInfo":{"status":"ok","timestamp":1692422384577,"user_tz":-330,"elapsed":4,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}}},"outputs":[],"source":["def preprocessing(text):\n","    data = pd.DataFrame([text], columns=['tweet'])\n","    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","    data[\"tweet\"] = data['tweet'].apply(lambda x: \" \".join(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) for x in x.split()))\n","    data[\"tweet\"] = data[\"tweet\"].apply(remove_punctuations)\n","    data[\"tweet\"] = data['tweet'].str.replace('\\d+', '', regex=True)\n","    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n","    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(ps.stem(x) for x in x.split()))\n","    return data[\"tweet\"]"]},{"cell_type":"code","execution_count":33,"id":"26d740ce-14be-47c4-a69e-e8333603a955","metadata":{"id":"26d740ce-14be-47c4-a69e-e8333603a955","executionInfo":{"status":"ok","timestamp":1692422382070,"user_tz":-330,"elapsed":333,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}}},"outputs":[],"source":["def vectorizer(ds, vocabulary):\n","    vectorized_lst = []\n","\n","    for sentence in ds:\n","        sentence_lst = np.zeros(len(vocabulary))\n","\n","        for i in range(len(vocabulary)):\n","            if vocabulary[i] in sentence.split():\n","                sentence_lst[i] = 1\n","\n","        vectorized_lst.append(sentence_lst)\n","\n","    vectorized_lst_new = np.asarray(vectorized_lst, dtype=np.float32)\n","\n","    return vectorized_lst_new"]},{"cell_type":"code","execution_count":15,"id":"ffb827c1-e6a6-43d9-a170-36f7012b30fd","metadata":{"id":"ffb827c1-e6a6-43d9-a170-36f7012b30fd","executionInfo":{"status":"ok","timestamp":1692420072138,"user_tz":-330,"elapsed":419,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}}},"outputs":[],"source":["def get_prediction(vectorized_text):\n","    prediction = model.predict(vectorized_text)\n","    if prediction == 1:\n","        return 'negative'\n","    else:\n","        return 'positive'"]},{"cell_type":"code","execution_count":53,"id":"c42b7daa-db43-4eee-89dd-36fcef84c3de","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c42b7daa-db43-4eee-89dd-36fcef84c3de","executionInfo":{"status":"ok","timestamp":1692423919593,"user_tz":-330,"elapsed":356,"user":{"displayName":"Ravindu Senadeera","userId":"01017905210180257585"}},"outputId":"a11a7913-ef84-4103-8428-4f992b1f500e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Error: The specified stopwords file does not exist.\n","Vocabulary loaded successfully.\n","negative\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]}],"source":["import pickle\n","import pandas as pd\n","import numpy as np\n","import re\n","from nltk.stem import PorterStemmer\n","\n","# Update the file paths to the correct ones\n","model_file_path = '/content/drive/MyDrive/EmotionAnalysisProject-main/static/model/model.pickle'\n","sw_file_path = '/content/drive/MyDrive/EmotionAnalysisProject-main/static/model/corpora/stopwords/english.txt'\n","vocab_file_path = '/content/drive/MyDrive/EmotionAnalysisProject-main/static/model/vocabulary.txt'\n","\n","# Load the pickled model\n","with open(model_file_path, 'rb') as f:\n","    model = pickle.load(f)\n","\n","# Load stopwords\n","try:\n","    with open(sw_file_path, 'r') as file:\n","        sw = file.read().splitlines()\n","    print(\"Stopwords loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"Error: The specified stopwords file does not exist.\")\n","\n","# Load vocabulary\n","try:\n","    vocab = pd.read_csv(vocab_file_path, header=None)\n","    tokens = vocab[0].tolist()\n","    print(\"Vocabulary loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"Error: The specified vocabulary file does not exist.\")\n","\n","ps = PorterStemmer()\n","\n","def preprocessing(txt, stopwords):\n","    data = pd.DataFrame([txt], columns=['tweet'])\n","    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","    data[\"tweet\"] = data['tweet'].apply(lambda x: \" \".join(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) for x in x.split()))\n","    data[\"tweet\"] = data[\"tweet\"].apply(remove_punctuations)\n","    data[\"tweet\"] = data['tweet'].str.replace('\\d+', '', regex=True)\n","    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))\n","    data[\"tweet\"] = data[\"tweet\"].apply(lambda x: \" \".join(ps.stem(x) for x in x.split()))\n","    return data[\"tweet\"]\n","\n","def vectorizer(txt, vocabulary):\n","    vectorized_lst = []\n","\n","    sentence_lst = np.zeros(len(vocabulary))\n","\n","    for i in range(len(vocabulary)):\n","        if vocabulary[i] in txt:\n","            sentence_lst[i] = 1\n","\n","    vectorized_lst.append(sentence_lst)\n","\n","    vectorized_lst_new = np.asarray(vectorized_lst, dtype=np.float32)\n","\n","    return vectorized_lst_new\n","\n","def get_prediction(prediction_prob):\n","    if prediction_prob >= 0.5:\n","        return 'negative'\n","    else:\n","        return 'positive'\n","\n","txt = \"highly recommanded\"\n","preprocessed_txt = preprocessing(txt, sw)\n","vectorized_txt = vectorizer(preprocessed_txt.iloc[0].split(), tokens)  # Access the text using iloc\n","prediction_prob = model.predict(vectorized_txt)\n","prediction = get_prediction(prediction_prob[0])  # Assuming prediction_prob is a single value\n","print(prediction)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}